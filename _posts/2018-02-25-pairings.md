---
title: Towards better food pairings?
categories:
  - Food
#tags:
#  - 
#  - 
#  - 
---

What goes well with strawberries? What with blue cheese?
When it comes to food pairings and cooking do you listen to your mouth or to data?

Many people have tried the latter, relying on data for a possibly more scientific approach.
One theory claims that two ingredients are made for each other when they share a number of 
volatile molecules, or specific important ones. It is the approach championed by Heston Blumhental 
and Herve This among others. Thanks to it humanity can now enjoy [chocolate with blue cheese](https://www.youtube.com/watch?v=_9Fi8ylaL4I),
and [mustard with mint](http://www.kookjegek.nl/2007/08/tgrwt-4-round-up/).
You can read more about it on the delightful [Khymos blog](https://blog.khymos.org/molecular-gastronomy/flavor-pairing/).


Another approach relies on large collection of recipes and on techniques borrowed
from natural language processing (NLP). Roughly speaking the idea is to treat lists of ingredients
as bag of words and extract word embeddings. It is expected that similar foods will end up close 
to each other in the latent space.


In the following I will reproduce them both and then proceed in proposing a third alternative which combines them.
It will produce a new vector representation of food. As a taste of it let me show you graphs representing two familiar recipes, each node is an ingredient and the edge's thickness corresponds to the 
strength of their pairing (or how well they go together). 

<figure class="half">
    <img src="/assets/images/pairings/chickenparm.png">
    <img src="/assets/images/pairings/brownie.png">
    <figcaption>Caption describing these two images.</figcaption>
</figure>

Is it enough to convince you that we are onto something?



Word embeddings are exciting but usually they tend to predict well know associations or trivial 
ones. (They tend to reinforce frequent pairings in the corpus and so on.)
You can be easily convinced of that by playing for a few minutes with the fantastic [Food2Vec]().
Looking at molecules, without guidance ... only an expert trained and with a lab. 
Moreover available datasets do not contain information on the amount contained in foods.

Can we find a better way to make use both recipe datasets and molecules one?
To answer that question I decided to use two publicly available sources:
- 1M recipe  
- website, which I scraped  . After cleaning I obtained a set of ~300 ingredients characterized
by ~1k molecules in total


## Shared Flavor Molecules 

So how can we find a list of molecules present in a given ingredient?
The biggest challenge is indeed to find a suitable dataset. Best ones are on books, hence difficult to scrape,
or not publicly available. For instance the paid service would not like to share its data.
I was lucky enough to find [FlavorDB](http://cosylab.iiitd.edu.in/flavordb/) and scraped it. 
After some cleaning I ended up with a dataset of 300+ ingredients, sharing in total ~1k volatile molecules. 

<figure class="half">
    <img width="220" src="/assets/images/pairings/zipfs.png">
    <figcaption>Ranking of flavour molecules by number of ingredients sharing them. 
    After an initial plateau of very common molecules we find a power law similar to what expected Zipf's law.</figcaption>
</figure>

Now finding pairings is trivial, ranking them by number of shared volatile molecules.
It is equivalent to map ingredients to a one hot encoded space of molecules, then take the dot product of two vectors.  
This is all we can do with just this dataset, there is no information on the quantity of molecules contained in each food,
how important they are for any given flavour profile and so on. 
Unfortunately the dataset itself is also very heterogeneous, which makes it less reliable.
Another limitation of this method is that two elements could share a large number of unimportant molecules hence its results should be taken
with a grain of salt (pun intended).

## Word Embeddings

For the second approach we need a large collection of recipes. I used the MIT dataset from here. 
It contains 1+ million recipes, including lists of ingredients. That's what we are looking for. 
I restricted each list to contain only elements appearing in the other dataset, to be able to compare results later.
Here is the distribution of the final number of ingredients per recipe
<figure class="half">
    <img width="160" src="/assets/images/pairings/n_ingredients.png">
    <figcaption>Distribution of the number of ingredients in recipes.</figcaption>
</figure>

In this case we use a word2vec method to assign a vector representation to ingredients. 
I used  [GenSim]() to train on the set of lists, treating them as bag of words. 
We can again compute the cosine similarity between to foods to discover how well they go together. 
The problem with this approach is that often it just highlights substitutions (fish for fish, tomatoes for other kind of tomatoes)
or very common pairings. While we are searching for something less accessible.

## Weighted molecular vectors 

Given that both methods above have pros and cons, can we combine them to get better results?
In principle existing recipes can inform us on which molecules in each ingredient are the most important. 
They should appear often as paired with others, if the initial hypothesis of sharing volatile molecules is correct.
Thus my idea is to weight the molecules of each ingredient. For each food I count how many times each of its molecules is shared with another ingredient in the same recipe
throughout the entire recipe collection. Then I adjust it by the logarithm of the inverse frequency of that molecule.
Hopefully in this way we highlight at the same time important molecules in food pairings in general, and those peculiar to each ingredient.

 
So once again we ended up with a vector representation and as before we compute a dot product to asses similarity.
But first to check whether we computed something meaningful I decided to take a list of fruits and through Principal Component Analysis 
visualize their relationship in the latent space. Here is the projection on the fist two major components

<figure >
    <img src="/assets/images/pairings/fruitPCA.png" style="width: 500px;">
    <figcaption> Principal component analysis for the weighted vector representations of common fruits. 
    Projection on the first two component plane.</figcaption>
</figure>

It makes sense, as you can readily see that there are some sort of berries, citrus and tropical fruit clusters.
Why banana is so close to strawberry though?

## Comparison

We are ready to compare pairing suggestions. I will show the best 10 pairings to a certain ingredient for each
method, here are te results for **squid**:
 
  
  
 Weighted    |     | N Molecules |  | Word Embedding |
  ----------- | ---- | ---------- | --- | ----------------- | ----
  scallop           |  0.55 | coffee         |  20 | scallop      |  0.72
  egg               |  0.54 | beer           |  20 | clam         |  0.69
  shrimp            |  0.52 | rice           |  19 | cod          |  0.63
  crab              |  0.47 | chocolate      |  18 | shrimp       |  0.58
  milk powder       |  0.46 | cocoa          |  18 | lemon grass  |  0.57
  cheese            |  0.44 | tea            |  18 | crayfish     |  0.53
  bonito            |  0.44 | potato         |  16 | crab         |  0.51
  wholewheat bread  |  0.43 | mushroom       |  15 | oyster       |  0.48
  sake              |  0.41 | peanut butter  |  15 | jasmine      |  0.47
  rum               |  0.4  | peanut         |  15 | okra         |  0.45


Of course I cheated choosing a good example which shows both strengths and weaknesses of each method.
Word embeddings give either predictable pairings or more of possible substitutes for our ingredient.
Viceversa number of shared molecules ends up giving interesting pairings but which ones should we trust? 
Weighted vectors give somewhat a combination of the two, apparently promoting squid-cheese (which is not that crazy) as the best 
least expected result. 

Another interesting example for squid:




You can find and explore the complete best 10 results on a CSV file uploaded [here](https://github.com/roundedup/food_pairings/blob/master/pairings.csv)
 on my GitHub, all the code I used is on the
[repo](https://github.com/roundedup) as well.
In the same depository there are also the complete vector representations for the three methods
in case anybody wants to play with them, and of course the code I used.
Bon Appetit!
 

## Wishlist

There is a lot of room for improvement here. 
First and foremost a better flavour molecule dataset, if anybody knows a good method to scrape a
PDF file it would be very helpful.
Then is there any other way of ranking molecules? 
My brute force approach assumed that all couples of ingredients have the same importance in a recipe, but
that's not clearly the same. Using a large dataset mitigates the problem but maybe there is a better way to find 
important pairs.



Now this


