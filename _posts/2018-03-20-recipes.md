---
title: What should I cook for dinner?
categories:
  - Food
#tags:
#  - 
#  - 
#  - 
---

It's time to make dinner. But what can I make with the few things left in the fridge?
When in search of inspiration there is nothing better than getting suggestions from your laptop.

That's the plan, very simple. Give in input a list of ingredients and get back an idea, the name of a dish 
that could possibly be made with them. Using a generative model we could even find new recipes
never discovered before, it could even help us find a dish to valorize one of those new food pairings we discovered 
last time.

## Data overview

How do we get there in practice? I decided to code an RNN, with a vector to sequence model.
Technically speaking I used as starting point the [seq2seq Keras](https://github.com/roundedup) code available here, adapting
parts of it and coding all the necessary additions. The final network architecture is relatively straightforward and,
as often the case, a picture is worth a thousand words so here is a cartoon of it

<figure >
    <img width="320" src="/assets/images/pairings/cartoon.png">
    <figcaption>Unrolled depiction of the RNN, with "chewy chocolate chip cookie" recipe.</figcaption>
</figure>


The input is a list of ingredients in the form of a one hot encoded vector. It is passed through a dense
layer to reduce its dimensionality from ~500 to 32 and used as the initial hidden state of a GRU with softmax activation. 
which takes the title of the recipe as input, trying to predict the title itself (with teacher mode enforced).
The title representation is word level, with a word encoding of dimension ~4000.
The whole network is trained on a list of almost 200k recipes, that I selected from the [MIT recipe dataset](https://github.com/roundedup)
after cleaning the ingredient list, mostly by requiring title length between 3 and 7 words, no uncommon words in the title.

My laptop was really slow so I switched to [Amazon Web Services](https://github.com/roundedup), this two guides were very useful.
Training took a few hours for a total of 200 epochs on a p2.xlarge instance. 
After training we can generate new recipe titles. Each word is chosen among the top 5 most probable ones predicted 
by the network and used as an input to predict the next one.  

## Results

With no further ado here are some results. First we can check that conditioning on common ingredient
lists returns reasonable titles. For instance given the chicken parm ingredients from the graph in the previous post
the suggestions are  
- Mozzarella Basil Stuffed Chicken Thighs
- Chicken Parmigiana Pie With Basil
- Mozzarella Parmesan Bruschetta With Chicken Breasts  
among others. For the other list in the previous post, the peanut butter brownies, the silicon chef recomends:
- Chewy Peanut Chocolate Cookies
- Peanut Butter Brownies In Chocolate Chip Sauce
- Double Fudge Cookies With Chocolate Peanut Butter
Not too bad!

Now we can try something more original, for instance the crab and chocolate combination from earlier.
Here is what my code offers for dinner:



I selected only three but of course there are also disastrous suggestions:

- Chocolate Crabmeat Pudding
- Hot Crab And Chocolate Soup


- Vegan Cashew And Chocolate Salad
- Raw Cashew And Chocolate Curry Sauce


- Chickpea Squid Salad In Garlic And Honey Oil With Parmesan Crumbs

- Grapefruit Tuna And Rice
- Grapefruit Tuna And Oat Bars With Honey Oil


- Of Those Back De Tails 
- Wrapped Of The Best To Your Day Cookies For Tacos Ever

## What's next?

Clearly there is a lot of room for improvement. Bigger training set and cleaner one are the first 
things to come to mind. It is clear from the results that there is some overtraining. 
Optimization of the architecture should help too.

It would be fun to put online an app to let everybody enjoy the results.
Finally it would be interesting to expand the code to generate not only titles but whole recipes with instructions.




## GitHub Repo

Code as always is on my GitHub [repo](https://github.com/roundedup)
